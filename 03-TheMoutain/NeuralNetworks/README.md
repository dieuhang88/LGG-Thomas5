# Neural Networks

This week's objective is very simple. It is dedicated to Neural Networks and how they function.

The holy grail would be to make a neural network by using NO MACHINE LEARNING LIBRARIES.

The exercise for this week is to perform a handwritten digit recogniser using the following dataset:
- https://www.kaggle.com/datasets/hojjatk/mnist-dataset

[This video series](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) (1-4) teaches you how neural networks function. He even shares the code.

This exercise can be quite mathy, so you can adopt one of two stategies:
1. You start by doing the exercise using easy libraries like SkLearn and then move on to Pytorch and then try to do it all from scratch using at most numpy.
2. You start by trying from scratch and divert to common frameworks later if you don't feel like it's working.

Resources:
- [Guy that does exactly this exercise](https://www.youtube.com/watch?v=w8yWXqWQYmU). He even flexes by timing himself ahaha.
- [Article about this exercise](https://medium.com/@ombaval/building-a-simple-neural-network-from-scratch-for-mnist-digit-recognition-without-using-7005a7733418)
- [Easy explanation to gradient descent](https://www.youtube.com/watch?v=sDv4f4s2SB8). TRIPLE BAM
- [Guy that builds a small GPT from scratch](https://www.youtube.com/watch?v=kCc8FmEb1nY). If you're done with the handwritten digits move to that.

After this week I want you to know:
- What is backpropagation
- What is gradient descent
- What are activation functions. And why we use them